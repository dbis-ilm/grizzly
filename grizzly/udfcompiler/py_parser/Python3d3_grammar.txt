/*
 * The MIT License (MIT)
 *
 * Copyright (c) 2014 Bart Kiers
 * Copyright (c) 2019 Robert Einhorn
 *
 * Permission is hereby granted, free of charge, to any person
 * obtaining a copy of this software and associated documentation
 * files (the "Software"), to deal in the Software without
 * restriction, including without limitation the rights to use,
 * copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following
 * conditions:
 *
 * The above copyright notice and this permission notice shall be
 * included in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
 * OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
 * HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
 * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 *
 * Project      : Python3-parser; an ANTLR4 grammar for Python 3
 *                https://github.com/bkiers/Python3-parser
 * Developed by : Bart Kiers, bart@big-o.nl
 *
 * Project      : an ANTLR4 grammar for Tiny Python without embedded actions
 *                https://github.com/antlr/grammars-v4/tree/master/python/tiny-python/tiny-grammar-without-actions
 * Developed by : Robert Einhorn, robert.einhorn.hu@gmail.com
 */

// Based on the Bart Kiers's ANTLR4 Python 3.3 grammar: https://github.com/bkiers/Python3-parser
// and the Python 3.3.7 Language Reference:             https://docs.python.org/3.3/reference/grammar.html

//Grammar used by udf compiler, to use it rename to Python3d3.g4

grammar Python3d3; // tiny version

tokens { INDENT, DEDENT }

//Note the indentation of code inside

@lexer::header{
from antlr4.Token import CommonToken
import re
import importlib
# Allow languages to extend the lexer and parser, by loading the parser dynamically
module_path = __name__[:-5]
language_name = __name__.split('.')[-1]
language_name = language_name[:-5]  # Remove Lexer from name
LanguageParser = getattr(importlib.import_module('{}Parser'.format(module_path)), '{}Parser'.format(language_name))
}

@lexer::members {
@property
def tokens(self):
    try:
        return self._tokens
    except AttributeError:
        self._tokens = []
        return self._tokens
@property
def indents(self):
    try:
        return self._indents
    except AttributeError:
        self._indents = []
        return self._indents
@property
def opened(self):
    try:
        return self._opened
    except AttributeError:
        self._opened = 0
        return self._opened
@opened.setter
def opened(self, value):
    self._opened = value
@property
def lastToken(self):
    try:
        return self._lastToken
    except AttributeError:
        self._lastToken = None
        return self._lastToken
@lastToken.setter
def lastToken(self, value):
    self._lastToken = value
def reset(self):
    super().reset()
    self.tokens = []
    self.indents = []
    self.opened = 0
    self.lastToken = None
def emitToken(self, t):
    super().emitToken(t)
    self.tokens.append(t)
def nextToken(self):
    if self._input.LA(1) == Token.EOF and self.indents:
        for i in range(len(self.tokens)-1,-1,-1):
            if self.tokens[i].type == Token.EOF:
                self.tokens.pop(i)
        self.emitToken(self.commonToken(LanguageParser.NEWLINE, '\n'))
        while self.indents:
            self.emitToken(self.createDedent())
            self.indents.pop()
        self.emitToken(self.commonToken(LanguageParser.EOF, "<EOF>"))
    next = super().nextToken()
    if next.channel == Token.DEFAULT_CHANNEL:
        self.lastToken = next
    return next if not self.tokens else self.tokens.pop(0)
def createDedent(self):
    dedent = self.commonToken(LanguageParser.DEDENT, "")
    dedent.line = self.lastToken.line
    return dedent
def commonToken(self, type, text, indent=0):
    stop = self.getCharIndex()-1-indent
    start = (stop - len(text) + 1) if text else stop
    return CommonToken(self._tokenFactorySourcePair, type, super().DEFAULT_TOKEN_CHANNEL, start, stop)
@staticmethod
def getIndentationCount(spaces):
    count = 0
    for ch in spaces:
        if ch == '\t':
            count += 8 - (count % 8)
        else:
            count += 1
    return count
def atStartOfInput(self):
    return Lexer.column.fget(self) == 0 and Lexer.line.fget(self) == 1
}

/*
 * parser rules
 */

// startRules:
file_input: (NEWLINE | stmt)* EOF;

stmt: simple_stmt | compound_stmt;

simple_stmt: small_stmt NEWLINE;
small_stmt: assignment_stmt | flow_stmt | print_stmt | func_call | grzly_stmt;
assignment_stmt: initialization | declaration | nontype_initialization | lst_assignment;
initialization: NAME ':' typ '=' expr;
declaration: NAME ':' typ;
nontype_initialization: GRZLYNAME '=' grzly_expr | NAME '=' expr;
lst_assignment: NAME OPEN_BRACK NUMBER CLOSE_BRACK '=' expr;

flow_stmt: break_stmt | continue_stmt | return_stmt | raise_stmt;
break_stmt: 'break';
continue_stmt: 'continue';
return_stmt: 'return' expr;
raise_stmt: 'raise' (NAME)?;

compound_stmt: if_stmt | while_stmt | for_stmt | exception_stmt;
if_stmt: 'if' ob_test ':' suite ('elif' ob_test ':' suite)* ('else' ':' suite)?;
while_stmt: 'while' ob_test ':' suite;
for_stmt: 'for' NAME 'in' (expr | rang | GRZLYNAME) ':' suite;
exception_stmt: 'try:' suite (except_stmt ':' suite)* ('else' ':' suite)? ('finally' ':' suite)?;
suite: simple_stmt | NEWLINE INDENT stmt+ DEDENT;

rang: 'range' OPEN_PAREN expr (SEP expr)? CLOSE_PAREN;
ob_test: test (log_op test)*;
test: expr (comp_op expr)*;
print_stmt: 'print' OPEN_PAREN expr CLOSE_PAREN;

log_op: 'and' | 'or' | 'not';
comp_op: '<' | '>' | '==' | '>=' | '<=' | '!=';
calc_op: '+' | '-' | '*' | '/' | '%' | '**' | '&' | '|' | '~' | '^' | '<<' | '>>'; 

expr:
   expr calc_op expr    
 | NAME                 
 | STRING               
 | FLOAT                
 | NUMBER               
 | BOOL  
 | list_expr     
 | list_dec            
 | db_reference         
 | typecast             
 | parenthesis_expr        
 | func_call  
;

grzly_expr:
   GRZLYNAME expr    
 | GRZLYNAME '[' (GRZLYNAME '.' NAME comp_op expr)* ']' ('.' func_call)?
 | GRZLYNAME '[' '[' STRING (SEP STRING)* ']' ']'
 | GRZLYNAME '.' expr 
 | expr '.' expr        
 | expr ',' expr
 | expr '(' expr ')'    
 | expr '[' expr ']'   
 | expr (comp_op expr) 
 | parenthesis_expr
 | brackets_expr 
 | db_reference
 ;

list_dec
: NAME OPEN_BRACK NUMBER CLOSE_BRACK
;

 list_expr
: OPEN_BRACK elems? CLOSE_BRACK
;

elems:
elem (SEP elem)*
;

elem:
  STRING 
| NUMBER 
| FLOAT 
| BOOL 
| NAME
;

parenthesis_expr: OPEN_PAREN expr CLOSE_PAREN;
brackets_expr: OPEN_BRACK expr CLOSE_BRACK;
func_call: (NAME '.')? NAME OPEN_PAREN params CLOSE_PAREN;
typecast: typ OPEN_PAREN expr CLOSE_PAREN;

params: (expr (SEP expr)*)*;

grzly_stmt: GRZLYNAME (expr)+;
except_stmt: 'except' expr? ('as' expr)?;

typ: 
    'int' 
 |  'str' 
 |  'list' 
 |  'float'
 |  'bool'
 |  'None'
 ;

db_reference
: NAME '.' NAME
;



/*
 * lexer rules
 */
SEP
: ','
;

STRING
 : STRING_LITERAL
 ;

NUMBER
 : INTEGER
 ;

FLOAT
 : DIGIT* '.' DIGIT*
 ;

 BOOL
 : 'True'
 | 'False'
;

INTEGER
 : DECIMAL_INTEGER
 ;

 NEWLINE
 : ( {self.atStartOfInput()}?   SPACES
   | ( '\r'? '\n' | '\r' | '\f' ) SPACES?
   )
   {
tempt = Lexer.text.fget(self)
newLine = re.sub("[^\r\n\f]+", "", tempt)
spaces = re.sub("[\r\n\f]+", "", tempt)
la_char = ""
try:
    la = self._input.LA(1)
    la_char = chr(la)       # Python does not compare char to ints directly
except ValueError:          # End of file
    pass
# Strip newlines inside open clauses except if we are near EOF. We keep NEWLINEs near EOF to
# satisfy the final newline needed by the single_put rule used by the REPL.
try:
    nextnext_la = self._input.LA(2)
    nextnext_la_char = chr(nextnext_la)
except ValueError:
    nextnext_eof = True
else:
    nextnext_eof = False
if self.opened > 0 or nextnext_eof is False and (la_char == '\r' or la_char == '\n' or la_char == '\f' or la_char == '#'):
    self.skip()
else:
    indent = self.getIndentationCount(spaces)
    previous = self.indents[-1] if self.indents else 0
    self.emitToken(self.commonToken(self.NEWLINE, newLine, indent=indent))      # NEWLINE is actually the '\n' char
    if indent == previous:
        self.skip()
    elif indent > previous:
        self.indents.append(indent)
        self.emitToken(self.commonToken(LanguageParser.INDENT, spaces))
    else:
        while self.indents and self.indents[-1] > indent:
            self.emitToken(self.createDedent())
            self.indents.pop()
    }
 ;

GRZLYNAME
: 'g_' ID_CONTINUE+
;

NAME
 : ID_START ID_CONTINUE*
 ;

STRING_LITERAL
 : '"' .*? '"'
 | '\'' .*? '\''
 ;

DECIMAL_INTEGER
 : NON_ZERO_DIGIT DIGIT*
 | '0'+
 ;

OPEN_PAREN : '(';
CLOSE_PAREN : ')';
OPEN_BRACK : '[';
CLOSE_BRACK : ']';
OPEN_BRACE : '{';
CLOSE_BRACE : '}';
ASSIGN_EQUAL: '=';

SKIP_
 : ( SPACES | COMMENT | LINE_JOINING ) -> skip
 ;

UNKNOWN_CHAR
 : .
 ;


/* 
 * fragments 
 */

fragment NON_ZERO_DIGIT
 : [1-9]
 ;

fragment DIGIT
 : [0-9]
 ;

fragment SPACES
 : [ \t]+
 ;

fragment COMMENT
 : '#' ~[\r\n\f]*
 ;

fragment LINE_JOINING
 : '\\' SPACES? ( '\r'? '\n' | '\r' | '\f' )
 ;

fragment ID_START
 : '_'
 | [A-Z]
 | [a-z]
 ;

fragment ID_CONTINUE
 : ID_START
 | [0-9]
 ;